[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Quarto Website Notes!",
    "section": "",
    "text": "Creating a Function Website that actually makes the URL Work!\n\nBuild\nCommit\nPush\n\nAdding Images\n\ndownload image\nin doc YAML\n\ndo “resources:”,\nindent then “- image/file/path/image.jpeg”\n\nreference image in doc with\n\n![](image.jpeg)"
  },
  {
    "objectID": "tabs/salmon_tummys.html",
    "href": "tabs/salmon_tummys.html",
    "title": "Diets of Juvenile Chinhook Salmon in PNW",
    "section": "",
    "text": "In Washington State, salmon are a big part of the water’s ecosystems and provide jobs and food to many people and animals. In elementary school most students spend time learning about the salmon cycle, even getting to raise their own fish. For this project we wanted to focus on something that was both interesting to us and held a bit of importance to our beloved home. Both of us were also intrigued by the prospect of this being someone’s job, going out and collecting this data. Over the course of this project we looked at the many different kinds of food that these juvenile chinhook salmon ate, how big they were,where these fish came from, and comparing the fish from hatcheries in various parts of the Pacific Northwest to the fish that were born in the wild. We examined the correlatory differences in the prey from both these sets of fish and the size of these fish.\n\n\n\nJuvenile Chinook salmon, Roger Tabor/USFWS, Public Domain, https://www.fws.gov/media/juvenile-chinook-salmon"
  },
  {
    "objectID": "tabs/salmon_tummys.html#weight-of-fish-and-prey-type",
    "href": "tabs/salmon_tummys.html#weight-of-fish-and-prey-type",
    "title": "Diets of Juvenile Chinhook Salmon in PNW",
    "section": "Weight of Fish and Prey Type",
    "text": "Weight of Fish and Prey Type\nIn this second graph we are exploring how the weight of a salmon correlates with the type of food it eats.\n\n\n\n\n\n\n\n\n\nThe fact that some of the heaviest fish ate plankton could be surprising, especially after cephalopods, a much more likely high median, is just above.\nThis graph is much like the previous one with all the same axes, though now the boxplots are split by fish that were raised in hatcheries vs. those in the wild.\n\n\n\n\n\n\n\n\n\nInterestingly, there is a strong correlation of wild fish being the heavier on average in all categories, and the hatchery raised median weights are much lighter, and nearly identical (~25 grams) across all categories, implying much more homogeneity in their group."
  },
  {
    "objectID": "tabs/salmon_tummys.html#length-of-fish-and-prey-type",
    "href": "tabs/salmon_tummys.html#length-of-fish-and-prey-type",
    "title": "Diets of Juvenile Chinhook Salmon in PNW",
    "section": "Length of Fish and Prey Type",
    "text": "Length of Fish and Prey Type\nFor our last size based graph we wanted to explore the length of the fish with how they correlate to the prey type eaten as compared to the weight graphs above. We also wanted to explore a graph with much more complication, and analyze it on many levels.\n\n\n\n\n\n\n\n\n\nThere’s a lot going on in this graph, but also some pretty interesting takeaways. The overall boxplots show that length has the same correlation with prey type as weight. The light gray beeswarm dots show that the hatchery fish once again are shorter than the whole group. Finally, the colored dots show the longest fish grouped by each hatchery."
  },
  {
    "objectID": "tabs/miniproj2.html",
    "href": "tabs/miniproj2.html",
    "title": "Mini Project #2",
    "section": "",
    "text": "After doing our amazing research on salmon we wanted to look at more data from Washington, and one thing that is becoming very prevalent is wildfires, both of us have been in very close proximity to wildfires and have seen the aftermath of many of them. We want to look at the amount of forest land that was lost or affected by forest fires. Thinking through this data we quickly found a data set of the land area affected by fires, we then realized that it would be interesting to see the amount of forested area that was affected by the fires and then also the total land area that was affected in the county as a whole."
  },
  {
    "objectID": "tabs/miniproj2.html#motivations-behind-our-data",
    "href": "tabs/miniproj2.html#motivations-behind-our-data",
    "title": "Mini Project #2",
    "section": "",
    "text": "After doing our amazing research on salmon we wanted to look at more data from Washington, and one thing that is becoming very prevalent is wildfires, both of us have been in very close proximity to wildfires and have seen the aftermath of many of them. We want to look at the amount of forest land that was lost or affected by forest fires. Thinking through this data we quickly found a data set of the land area affected by fires, we then realized that it would be interesting to see the amount of forested area that was affected by the fires and then also the total land area that was affected in the county as a whole."
  },
  {
    "objectID": "tabs/miniproj2.html#research-questions",
    "href": "tabs/miniproj2.html#research-questions",
    "title": "Mini Project #2",
    "section": "Research Questions",
    "text": "Research Questions\nWe are interested in looking at the total land area that is affected by forest fires in our home state. We wanted to not only look at the forest land that was effected by also the amount of land area that is forested and then also affected by the fire.\n\n# Our first table came from wikipedia, which is an allowed source\nis_valid_robotstxt(\"https://en.wikipedia.org/wiki/List_of_Washington_wildfires\")\n\n[1] TRUE\n\n#reading the html of the website\nwildfires &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_Washington_wildfires\")\n\n#scraping the table\nwildfiretables &lt;- html_nodes(wildfires, css = \"table\") \n\n\n#our first raw set of tables\ntables &lt;- (html_table(wildfiretables, header = TRUE, fill = TRUE))\n\nknitr::kable(head(tables))\n\n\n# Since we had so many tables from one scrape to use, we created a small \n# function to choose the table from the list using its subset number, cleaned \n# the names, remove unnecessary columns, and rename a common variables. Due to \n# inconsistency, all variables were set set as character and then parsed for \n# numbers.\n\ncleaninggg &lt;- function(table, i) {\n  html_table(table, header = TRUE, fill = TRUE)[[i]]|&gt; \n    janitor::clean_names() |&gt;\n    select(-notes, -image, -injuries, -complex_name) |&gt;\n    mutate(across(c(structureslost, size_acres), as.character),\n           across(c(structureslost, size_acres), parse_number)) |&gt;\n    rename(\"fire_size_acres\" = \"size_acres\")\n}\n\n# Running the function for each of the times to \n# pull the data out of the list from wikipedia into 5 (nearly) uniform datasets\ntwenty &lt;- cleaninggg(wildfiretables, 2) |&gt; rename(\"start_date\" = \"start_date_cause\")\nten &lt;- cleaninggg(wildfiretables, 3) \nthousand &lt;- cleaninggg(wildfiretables, 4)\nnines &lt;- cleaninggg(wildfiretables, 5) \nminors &lt;- cleaninggg(wildfiretables, 6) \n\n# Binds all of the major fires into one dataset and removes deaths for \n# consistency with the minor fires\nmajors &lt;- rbind(twenty, ten, thousand, nines) |&gt; select(-deaths)\n\n# Adds a column that identifies is a fire was major or minor\nminors['fire_type'] = \"Minor\"\nmajors['fire_type'] = \"Major\"\n\n# Joins all fires together\nfires &lt;- rbind(majors, minors)\n\nknitr::kable(head(fires))\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nfire_name\ncounty\nstart_date\nfire_size_acres\nstructureslost\nfire_type\n\n\n\n\n2024\nBeam Road Fire[2]\nYakima\nJune 15\n8542\n0\nMajor\n\n\n2024\nBig Horn Fire[3][4]\nKlickitat\nJuly 22, unknown\n51569\n0\nMajor\n\n\n2024\nBlack Canyon Fire[5]\nYakima\nJuly 22, unknown\n9211\n0\nMajor\n\n\n2024\nCougar Creek Fire[6][7]\nAsotin & Garfield\nJuly 15, unknown\n20699\n4\nMajor\n\n\n2024\nPioneer Fire[8]\nChelan\nJune 8, human caused\n36763\n0\nMajor\n\n\n2024\nRetreat Fire[9][10]\nYakima\nJuly 23, cause unknown\n44588\n5\nMajor\n\n\n\n\n\n\n# As most major fires burn throughout forests, we wanted to add in a dataset \n# about forest coverage per county, we were planning to make a for-loop for \n# this, but all of the websites we tried to scrape weren't reading the actual \n# number as it was stored as an image? So we found this website that stores it\n# all as a list\nis_valid_robotstxt(\"https://data.workingforests.org/#\")\n\n[1] TRUE\n\nsession &lt;- bow(\"https://data.workingforests.org/#\")\n\n# Scraped the county names as one list\ncounty_title &lt;- scrape(session) |&gt;\n  html_nodes(\".countyName\") |&gt;\n  html_text()\nknitr::kable(head(county_title))\n\n\n\n\nx\n\n\n\n\nStatewide\n\n\nAdams County\n\n\nAsotin County\n\n\nBenton County\n\n\nChelan County\n\n\nClallam County\n\n\n\n\n# Scraped the forest coverage as another list\nforest_cov &lt;- scrape(session) |&gt;\n  html_nodes(\".dataValueEmphasized\") |&gt;\n  html_text() \nknitr::kable(head(forest_cov))\n\n\n\n\nx\n\n\n\n\n22,983,438\n\n\n1,452\n\n\n103,022\n\n\n351\n\n\n1,392,891\n\n\n1,034,606\n\n\n\n\n# Brought the 2 lists together as one tibble with 2 columns, removed \" County\"\n# from name to synchronize with main table\nforest_cover &lt;- tibble(county = county_title, \n                    forest_coverage_acres = forest_cov) |&gt;\n  mutate(county = str_remove(county, \" County\"),\n         forest_coverage_acres = parse_number(forest_coverage_acres))\n\n# Joins this forest coverage with our fire data by county. For ease of analysis\n# at this stage without knowing string analysis in detail (yet!), we removed all\n# rows that contained 2 counties by dropping NA's in forest coverage. This way \n# all rows should have a complete collection of county name, forest size, and \n# fire size. \nfullfires &lt;- fires |&gt; left_join(forest_cover) |&gt;\n  drop_na(forest_coverage_acres) \n\nknitr::kable(head(fullfires))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nfire_name\ncounty\nstart_date\nfire_size_acres\nstructureslost\nfire_type\nforest_coverage_acres\n\n\n\n\n2024\nBeam Road Fire[2]\nYakima\nJune 15\n8542\n0\nMajor\n1201021\n\n\n2024\nBig Horn Fire[3][4]\nKlickitat\nJuly 22, unknown\n51569\n0\nMajor\n516397\n\n\n2024\nBlack Canyon Fire[5]\nYakima\nJuly 22, unknown\n9211\n0\nMajor\n1201021\n\n\n2024\nPioneer Fire[8]\nChelan\nJune 8, human caused\n36763\n0\nMajor\n1392891\n\n\n2024\nRetreat Fire[9][10]\nYakima\nJuly 23, cause unknown\n44588\n5\nMajor\n1201021\n\n\n2023\nGray Fire[15]\nSpokane\n\n10085\n259\nMajor\n318506\n\n\n\n\n\n\n# Lastly, we also thought it would be good to include the size of the counties \n# themselves as a comparison to the size of the forest its fires, so we scraped \n# this table\ncounties &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_counties_in_Washington\")\ncountytable &lt;- html_nodes(counties, css = \"table\") \n\n# This identifies the table we want, cleans the names, removes part of the name\n# ' County' for consistency, parses the sq. mi. and converts it to acres, and\n# selects just county and county size\n\ncountysize &lt;- html_table(countytable, header = TRUE, fill = TRUE)[[2]] |&gt; \n  janitor::clean_names() |&gt;\n  mutate(county = str_remove(county, \" County\"),\n         county_size_acres = parse_number(land_area_11) * 640) |&gt;\n    select(county, county_size_acres)\n\n# Finally ! We join this last table with the main dataset\nfinal_fires &lt;- fullfires |&gt; left_join(countysize)\n\nknitr::kable(head(final_fires))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nfire_name\ncounty\nstart_date\nfire_size_acres\nstructureslost\nfire_type\nforest_coverage_acres\ncounty_size_acres\n\n\n\n\n2024\nBeam Road Fire[2]\nYakima\nJune 15\n8542\n0\nMajor\n1201021\n2749440\n\n\n2024\nBig Horn Fire[3][4]\nKlickitat\nJuly 22, unknown\n51569\n0\nMajor\n516397\n1198080\n\n\n2024\nBlack Canyon Fire[5]\nYakima\nJuly 22, unknown\n9211\n0\nMajor\n1201021\n2749440\n\n\n2024\nPioneer Fire[8]\nChelan\nJune 8, human caused\n36763\n0\nMajor\n1392891\n1868800\n\n\n2024\nRetreat Fire[9][10]\nYakima\nJuly 23, cause unknown\n44588\n5\nMajor\n1201021\n2749440\n\n\n2023\nGray Fire[15]\nSpokane\n\n10085\n259\nMajor\n318506\n1128960"
  },
  {
    "objectID": "tabs/miniproj2.html#future-uses-of-this-data",
    "href": "tabs/miniproj2.html#future-uses-of-this-data",
    "title": "Mini Project #2",
    "section": "Future Uses of this Data",
    "text": "Future Uses of this Data\nFor future uses of this data we have a lot of things that we want to clean with string functions. We were also looking into census data for each county in Washington, which would be interesting to see if there is higher population in a county that has more forest fire activity. It would also be interesting to add spatial data to this to map the percentage of forest area affected by fires or other percentage maps."
  },
  {
    "objectID": "tabs/miniproj1.html",
    "href": "tabs/miniproj1.html",
    "title": "Mini Project #1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(tmap)\nlibrary(maps)\nlibrary(viridis)\nlibrary(htmltools)\nlibrary(glue)\nlibrary(leaflet)\n\nus_states &lt;- map_data(\"state\")\n\nfrogs &lt;- read.csv(\"~/Desktop/15/SDS264/data/most_common_frog.csv\") |&gt;\n  na.omit()\n\nforests &lt;- read.csv(\"~/Desktop/15/SDS264/data/forests.csv\") |&gt;\n  mutate('forest_cover' = forest_area/land_area * 100)\n\nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")"
  },
  {
    "objectID": "tabs/miniproj1.html#importing-data",
    "href": "tabs/miniproj1.html#importing-data",
    "title": "Mini Project #1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(tmap)\nlibrary(maps)\nlibrary(viridis)\nlibrary(htmltools)\nlibrary(glue)\nlibrary(leaflet)\n\nus_states &lt;- map_data(\"state\")\n\nfrogs &lt;- read.csv(\"~/Desktop/15/SDS264/data/most_common_frog.csv\") |&gt;\n  na.omit()\n\nforests &lt;- read.csv(\"~/Desktop/15/SDS264/data/forests.csv\") |&gt;\n  mutate('forest_cover' = forest_area/land_area * 100)\n\nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")"
  },
  {
    "objectID": "tabs/miniproj1.html#data-source",
    "href": "tabs/miniproj1.html#data-source",
    "title": "Mini Project #1",
    "section": "Data Source",
    "text": "Data Source"
  },
  {
    "objectID": "tabs/miniproj1.html#joining-with-polygons",
    "href": "tabs/miniproj1.html#joining-with-polygons",
    "title": "Mini Project #1",
    "section": "Joining with Polygons",
    "text": "Joining with Polygons\n\nforests_static &lt;- forests |&gt;\n  mutate(state = str_to_lower(state)) |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) \n\n\nfrogs_static &lt;- frogs |&gt;\n  mutate(state = str_to_lower(state)) |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\"))"
  },
  {
    "objectID": "tabs/miniproj1.html#static-mapping",
    "href": "tabs/miniproj1.html#static-mapping",
    "title": "Mini Project #1",
    "section": "Static Mapping",
    "text": "Static Mapping\n\nforests_static |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n  geom_polygon(aes(fill = forest_cover), color = \"black\", linewidth = 0.2) + \n  labs(fill = \"% Forest Coverage\", title = \"Percent of Forest Coverage in each US State\", x = \"Longitude\", y = \"Latitude\", caption = \"Data Source: USDA Forest Service FIA Annual Report\") +\n  coord_map() +\n  scale_fill_viridis(option = \"viridis\", direction = -1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nalt text: This is a choropleth map of the United States that is looking a the percentage of land area that is covered by forest in each state. The x-axis is the longitude values of the United States which contain values -120 - -80. The y-axis the latitude values of the United States which contain values 25-50. This map is colored by the percentage forest covereage with the most coverage being a dark purple while the least coverage is a bright yellow. From this map we can see that states in the East have a higher percentage of forest coverage while states in the middle have a much lower percent forest coverage, then the west coast has a medium amount of forest coverage.\n\nfrogs_static |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n  geom_polygon(aes(fill = frog), color = \"black\", linewidth = 0.2) + \n  labs(fill = \"Frog Common Name\", title = \"Most Observed Threatened Frog in Each State\", x = \"Longitude\", y = \"Latitude\", caption = \"Data Source: https://www.inaturalist.org/observations?place_id=1&subview=map&taxon_id=25473&threatened\") +\n  scale_fill_manual(values = c(\"#800000\", \n                               \"#9A6324\", \n                               \"#808000\", \n                               \"#469990\", \n                               \"#000075\", \n                               \"#000000\", \n                               \"#e6194B\", \n                               \"#f58231\", \n                               \"#ffe119\", \n                               \"#bfef45\", \n                               \"#3cb44b\", \n                               \"#42d4f4\", \n                               \"#4363d8\",\n                               \"#911eb4\", \n                               \"#f032e6\", \n                               \"#fabed4\", \n                               \"#ffd8b1\",\n                               \"#fffac8\",\n                               \"#aaffc3\")) +\n  coord_map() +\n  theme_minimal()"
  },
  {
    "objectID": "tabs/miniproj1.html#joining-for-interactive-polygons",
    "href": "tabs/miniproj1.html#joining-for-interactive-polygons",
    "title": "Mini Project #1",
    "section": "Joining for Interactive Polygons",
    "text": "Joining for Interactive Polygons\n\nstates &lt;- states |&gt;\n  filter(!(name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\"))) |&gt;\n  select(\"name\", \"geometry\")\n\nfrogs_interactive &lt;- frogs |&gt;\n  right_join(states, by = c(\"state\" = \"name\"))\n\n\nforest_interactive &lt;- forests |&gt;\n  right_join(states, by = c(\"state\" = \"name\"))"
  },
  {
    "objectID": "tabs/miniproj1.html#interactive-mapping",
    "href": "tabs/miniproj1.html#interactive-mapping",
    "title": "Mini Project #1",
    "section": "Interactive Mapping",
    "text": "Interactive Mapping\n\nforest_sf &lt;- st_as_sf(forest_interactive) |&gt;\n  mutate(forest_cover = trunc(forest_cover))\n\npal &lt;- colorNumeric(\"Greens\", domain = forest_sf$forest_cover)\n\nforest_sf &lt;- forest_sf |&gt;\n  mutate(labels = str_c(state, \": \", forest_cover, \"% forest cover\"))\n\nlabels &lt;- lapply(forest_sf$labels, HTML)\nleaflet(forest_sf) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addProviderTiles(\"Esri.WorldTopoMap\") |&gt;\n  addPolygons(\n    fillColor = ~pal(forest_cover),\n    weight = 2,\n    opacity = 1,\n    color = \"black\",\n    fillOpacity = 0.6,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"pink\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"12px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, title = \"% Forest Coverage\", values = ~forest_cover, opacity = 0.7, position = \"bottomright\") |&gt;\n  addScaleBar(position = \"bottomleft\") |&gt;\n  addPopups(-95, 50, \"Percentage of Forest Cover in Each State\",\n              options = popupOptions(closeOnClick = FALSE))\n\n\n\n\n\n\nfrog_sf &lt;- st_as_sf(frogs_interactive) \npal &lt;- colorFactor(c(\"#800000\", \n                               \"#9A6324\", \n                               \"#808000\", \n                               \"#469990\", \n                               \"#000075\", \n                               \"#000000\", \n                               \"#e6194B\", \n                               \"#f58231\", \n                               \"#ffe119\", \n                               \"#bfef45\", \n                               \"#3cb44b\", \n                               \"#42d4f4\", \n                               \"#4363d8\",\n                               \"#911eb4\", \n                               \"#f032e6\", \n                               \"#fabed4\", \n                               \"#ffd8b1\",\n                               \"#fffac8\",\n                               \"#aaffc3\"), domain = frog_sf$frog)\n\nfrog_sf &lt;- frog_sf |&gt;\n  mutate(labels = str_c(\"The \", frog, \" is the most observed threatened frog in \", state))\n\nlabels &lt;- lapply(frog_sf$labels, HTML)\n\nleaflet(frog_sf) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addProviderTiles(\"Esri.WorldTopoMap\") |&gt;\n  addPolygons(\n    fillColor = ~pal(frog),\n    weight = 2,\n    opacity = 1,\n    color = \"black\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"pink\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"12px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, title = \"Frogs Observed\", values = ~frog, opacity = 0.7, position = \"bottomright\") |&gt;\n  addScaleBar(position = \"bottomleft\")"
  },
  {
    "objectID": "tabs/mp4.html",
    "href": "tabs/mp4.html",
    "title": "Mini Project #4",
    "section": "",
    "text": "Introduction\nOne of my favorite bands is called Illuminati Hotties, what I love about them is that all the songs are very different and unique. The main artist is named Sarah Tudzin and in her professional life she is a music producer for many much more popular artists. She also writes and produces her own music and what I think is really cool about it is she just has fun making a bunch of cool and unique sounds, that aren’t dictated by someone else.\nSome of my friends and I went to a concert for another artist called Pom Pom Squad almost 6 years ago at this point where we were first introduced to her and have been hooked ever since, one of the things we always talk about is the uniqueness of her music and the interesting and sometimes slightly disturbing lyrics she uses in her songs. This led me to wanting to do a text analysis of her song lyrics.\n\n\n\nData!\nThere was not a data set of her song lyrics as she is not a very famous artist so i made the maybe silly decision to scrape all the lyrics off the web myself and make my own data set. I did all of this in a document called MP4_cleaning.qmd because it was a lot of code that was not important to the actual project results, it is however linked in my github and on this website page.\nWhat I did was used the web page scraping functions that we made in class to make a similar function that I could input the names of the songs into. I first tried to do all the songs at once but was running into issues with my IP address getting blocked from requesting too much data so I had to break down the songs into smaller requests and I made 10 individual csv files that I then bind_row() into one full data set. This however was only the names of the songs and the lyrics and I wanted an album name which was a little more complicated because the website I used to get the lyrics did not have the album name on the page that the song lyrics were on. So I just did a bunch of if else statements to add the new column. This definitely was not the most efficient method of doing this but its what I did because it was more familiar for a part of the project that was completely unnecessary.\n\n\n\n\n\nSong\nword\nalbum\n\n\n\n\n(You’re Better) Than Ever\nall\nKiss Yr Frenemies\n\n\n(You’re Better) Than Ever\nmy\nKiss Yr Frenemies\n\n\n(You’re Better) Than Ever\nfavorite\nKiss Yr Frenemies\n\n\n(You’re Better) Than Ever\nsocks\nKiss Yr Frenemies\n\n\n(You’re Better) Than Ever\nare\nKiss Yr Frenemies\n\n\n(You’re Better) Than Ever\ngetting\nKiss Yr Frenemies\n\n\n\n\n\nThis is what the data set looks like after I read it into this document and did some final cleaning!\n\nIlluminati Hotties have 5 albums and a couple singles that I am analyzing together as one “album”.\n\n\n\n\n\nLet Me Do One More\n\n\n\n\n\n\nKiss Yr Frenemies\n\n\n\n\n\n\nFree I.H: This is Not the One You’ve Been Waiting For\n\n\n\n\n\n\nNickel on the Fountain Floor\n\n\n\n\n\n\nPower\n\n\n\n\n\n\n\nQuestions\nSome things I want to explore with in this data set are:\n\nMost common words in general, in each song and in each album\nSentient analysis of each album\nNetwork Graph and Correlation\n\n\n\n\nWord Counts\nI first wanted to just look at the most common word in all of her songs in general and found what was expected, words like “I”, “you” and “the” were the most commonly used words.\n\n\n\nMost Common Words for All Songs\n\n\nalbum\nword\nn\n\n\n\n\nPower\ni\n121\n\n\nlet me do one more\ni\n113\n\n\nKiss Yr Frenemies\ni\n100\n\n\nSingle\ni\n70\n\n\nFREE I.H: This Is Not the One You’ve Been Waiting For\nyou\n59\n\n\nNickel on the Fountain Floor\nyou\n17\n\n\n\n\n\n\n\nTo look at just the most common useful words I anti_joined my data set with the set of of smart stop words. Also some of the most common words are not full words and are sounds as words, so I filtered them out using str functions and then looked at the most common words in each album again.\n\n\n\nMost Common Smart Word for Each Album\n\n\nalbum\nword\nn\n\n\n\n\nFREE I.H: This Is Not the One You’ve Been Waiting For\ncontent\n17\n\n\nKiss Yr Frenemies\ntime\n15\n\n\nNickel on the Fountain Floor\n777\n7\n\n\nPower\nlove\n36\n\n\nSingle\nmiss\n11\n\n\nlet me do one more\nwanna\n23\n\n\n\n\n\n\n\n\n\n\nMost Common Word for Each Song\n\n\nSong\nword\nn\n\n\n\n\nYSL\nplay\n20\n\n\ncontent//bedtime\ncontent\n17\n\n\nI Would Like, Still Love You\nlove\n16\n\n\nMMMOOOAAAAAYAYA\nyeah\n16\n\n\nfreequent letdown\nletting\n16\n\n\nu v v p\nhide\n16\n\n\nJoni: LA's No. 1 Health Goth\nbet\n15\n\n\nfree ppls\nfree\n14\n\n\nPatience\ntime\n13\n\n\nWATTBL\ntime\n13\n\n\n(You're Better) Than Ever\nahh\n12\n\n\nSleeping In\nsleeping\n12\n\n\nb yr own b\nfine\n12\n\n\nCheap Shoes\nwear\n10\n\n\nFalling In Love With Somebody Better\nlove\n10\n\n\nI Wanna Keep Yr Dog\ndog\n10\n\n\nYou Are Not Who You Were\nhand\n10\n\n\nsuperiority complex (big noise)\nreal\n10\n\n\nPressed 2 Death\nbad\n9\n\n\nPower\npower\n8\n\n\nTruck\ngive\n8\n\n\n777\n777\n7\n\n\nDecember\ndecember\n7\n\n\nKickflip\nday\n6\n\n\nPool Hopping\nhoppin\n6\n\n\nShape Of My Hands\nthinking\n6\n\n\nThe L\nlove\n6\n\n\nThe Sway\nleaning\n6\n\n\nWreck My Life\nlife\n6\n\n\nboi\nboi\n6\n\n\nDeclutter\nanymore\n5\n\n\nPaying Off The Happiness\npaying\n5\n\n\nwill i get cancelled\nman\n5\n\n\nCuff\ncuff\n4\n\n\nEverything Changes\nlose\n4\n\n\nProtector\nprotector\n4\n\n\nThe Rules\nread\n4\n\n\nThreatening Each Other re: Capitalism\nahh\n4\n\n\nThrow (Life Raft)\nback\n4\n\n\nppl plzr\nbrand\n4\n\n\nFor Cheez (My Friend, Not The Food)\neverything's\n3\n\n\nKiss Yr Frenemies\nwanted\n3\n\n\nKnead\nknead\n3\n\n\nRot\nball\n3\n\n\nSandwich Sharer\nback\n3\n\n\nWhat's the Fuzz\nblinder\n3\n\n\nCan't Be Still\nmoving\n2\n\n\nDidn't\nunenthused\n2\n\n\nGrowth\npretend\n2\n\n\nK - HOT AM 818\nhot\n2\n\n\nToasting\nbiting\n2\n\n\nfree dumb\nfucking\n2\n\n\nmelatonezone\nbarely\n2\n\n\nreasons 2 live\ngood\n2\n\n\n\n\n\n\n\nGraphs to visualize the most common words!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis\nI am really curious about the sentiment analysis of her songs because they lyrics are seemingly random and make not a lot of sense on their own. Also compared to the vibe of the song they sentiment seems to be all over the place. Something I would love to do in the future is add variables that relate to the “vibe” of the song similar to the Spotify data that we have used, there is an energy variable and a couple others that would be interesting to look at compared to the actual text sentiment analysis.\n\n\n\nEmotion of Each Album\n\n\nalbum\nsentiment\nn\n\n\n\n\nFREE I.H: This Is Not the One You’ve Been Waiting For\ntrust\n56\n\n\nKiss Yr Frenemies\nanticipation\n63\n\n\nNickel on the Fountain Floor\nsadness\n15\n\n\nPower\njoy\n59\n\n\nSingle\nanticipation\n33\n\n\nlet me do one more\nfear\n48\n\n\n\n\n\n\n\n\nSentiment Score of Each Album\n\n\nalbum\nvalue\nn\n\n\n\n\nFREE I.H: This Is Not the One You’ve Been Waiting For\n1\n37\n\n\nKiss Yr Frenemies\n2\n32\n\n\nNickel on the Fountain Floor\n-2\n8\n\n\nPower\n3\n44\n\n\nSingle\n-2\n28\n\n\nlet me do one more\n-1\n31\n\n\n\n\n\n\n\n\nPositive vs. Negative Sentiment of Each Album\n\n\nalbum\nsentiment\nn\n\n\n\n\nFREE I.H: This Is Not the One You’ve Been Waiting For\npositive\n85\n\n\nKiss Yr Frenemies\nnegative\n61\n\n\nNickel on the Fountain Floor\nnegative\n16\n\n\nPower\nnegative\n111\n\n\nSingle\nnegative\n59\n\n\nlet me do one more\nnegative\n84\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelations\nI also wanted to look at the correlations between some of the most common words in the songs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFun Fact!!\nIn the song “Pool Hopping” the first verse goes:\nIn every life there is a bell One rounded curve of time or tell I’m on the left half looking Over, over, over, uh-huh\nReferencing being on the left side of a a bell curve! That’s statistical and semi-relevant!"
  }
]